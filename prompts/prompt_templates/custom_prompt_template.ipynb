{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prompt Template\n",
    "---\n",
    "\n",
    "Vamos a suponer que queremos que nuestro LLM (Larga Language Model) genere descripciones para una funcion al cual proprocionamos su nombre.\n",
    "\n",
    "Para llevar a cabo esta tarea, necesitamos crear un \"prompt\" y su respesctivo \"template\", donde proporcionemos el nombre de la funcion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por qué se necesitan plantillas de \"prompts\" personalizados?\n",
    "----\n",
    "\n",
    "LangChain proporciona un conjunto \"prompts tempaltes\" que se pueden usar para una variedad de tareas. Sin embargo, puede haber casos en los que las plantillas de \"prompts\" predeterminadas no satisfagan sus necesidades. Por ejemplo, es posible que desee crear una plantilla de mensajes con instrucciones dinámicas específicas para su modelo de lenguaje. En tales casos, puede crear una plantilla personalizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando una plantilla de prompt personalizada (custom prompt template)\n",
    "---\n",
    "\n",
    "Escencialmente hay dos tipos distintos de plantilla disponibles:\n",
    "\n",
    "- StringPromptTemplate\n",
    "- ChatPromptTemplates\n",
    "\n",
    "**StringPromptTemplate** proporciona un mensaje simple en formato de cadena, mientras que **ChatPromptTemplates** produce un mensaje más estructurado para usar con una API de chat.\n",
    "\n",
    "En esta guia, crearemos un prompt personalizado (custom prompt) usando **StringPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "Given the function name and source code, generate an English language explanation of the funciont.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "===\n",
    "{source_code}\n",
    "===\n",
    "Explanation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Los prompts preferentemente debemos crearlos en ingles, ya que "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input varaibles are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable\")\n",
    "        return v\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompto to be sent to the language model\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, \n",
    "            source_code=source_code\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de uso\n",
    "---\n",
    "\n",
    "Ahora que hemos creado una plantilla de mensajes personalizada, podemos usarla para generar mensajes para nuestra tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function name and source code, generate an English language explanation of the funciont.\n",
      "Function Name: get_source_code\n",
      "Source Code:\n",
      "===\n",
      "def get_source_code(function_name):\n",
      "    # Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "===\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generaremos un prompt para la funcion \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Holochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
