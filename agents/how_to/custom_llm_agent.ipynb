{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente LLM Personalizado\n",
    "---\n",
    "\n",
    "Este cuaderno explica cómo crear su propio agente LLM personalizado.\n",
    "\n",
    "Un agente LLM consta de tres partes:\n",
    "\n",
    "- `PromptTempalte`: Esta es la plantilla de indicaciones que se puede utilizar para indicarle al modelo de lenguaje qué hacer\n",
    "- `LLM`: Este es el modelo de lenguaje que impulsa al agente.\n",
    "- `stop sequence`: Le indica al LLM que deje de generar tan pronto como se encuentre esta cadena\n",
    "- `OutputParser`: Determina cómo analizar la salida de LLM en un objeto AgentAction o AgentFinish.\n",
    "\n",
    "El Agente LLM se utiliza en un `AgentExecutor`. Este `AgentExecutor` puede considerarse en gran medida como un bucle que:\n",
    "\n",
    "1. Pasa la entrada del usuario y cualquier paso previo al Agente (en este caso, el Agente LLM)\n",
    "2. Si el Agente devuelve un `AgentFinish`, luego devuélvalo directamente al usuario.\n",
    "3. Si el Agente devuelve una `AgentAction`, úsela para llamar a una herramienta y obtener una `Observation`.\n",
    "4. Repita, pasando `AgentAction` y `Observation` nuevamente al Agente hasta que se emita un `AgentFinish`.\n",
    "\n",
    "`AgentAction` es una respuesta que consta de `acción` y `action_input`. `action` se refiere a qué herramienta usar y `action_input` se refiere a la entrada a esa herramienta. `log` también se puede proporcionar como más contexto (que se puede usar para registrar, rastrear, etc.).\n",
    "\n",
    "`AgentFinish` es una respuesta que contiene el mensaje final que se enviará al usuario. Esto debe usarse para finalizar la ejecución de un agente.\n",
    "\n",
    "En este cuaderno explicamos cómo crear un agente LLM personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el entorno\n",
    "---\n",
    "\n",
    "Realizar las importaciones necesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "from typing import List, Union\n",
    "import re\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar herramienta (tools)\n",
    "---\n",
    "\n",
    "Configure cualquier herramienta (`tool`) que el agente quiera utilizar. \n",
    "\n",
    "Puede ser necesario incluir esto en el `prompt` (para que el agente sepa cómo utilizar estas herramientas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plantilla de Prompt (Prompt Template)\n",
    "---\n",
    "\n",
    "Esto le indica al agente qué hacer. Generalmente, la plantilla debe incorporar:\n",
    "\n",
    "- `tools`: a qué herramientas tiene acceso el agente y cómo y cuándo llamarlas.\n",
    "- `intermediate_steps`: Estas son tuplas de pares anteriores (`AgentAction`, `Observation`). Por lo general, estos no se pasan directamente al modelo, pero el `prompt template` les da formato de una manera específica.\n",
    "- `input`: entrada genérica del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak.\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        \n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizador de salida (Output parser)\n",
    "---\n",
    "\n",
    "El analizador de salida (output parser) es responsable de analizar la salida de LLM en `AgentAction` y `AgentFinish`. Por lo general, esto depende en gran medida del mensaje utilizado.\n",
    "\n",
    "Aquí es donde puede cambiar el análisis para realizar reintentos, manejar espacios en blanco, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the \"Action\" and \"Action Input\"\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el LLM\n",
    "---\n",
    "\n",
    "¡Elija el LLM que desea utilizar!\n",
    "\n",
    "Para este ejemplo usaremos OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir la secuencia de parada (stop sequence)\n",
    "---\n",
    "\n",
    "Esto es importante porque le indica al `LLM` cuándo detener la generación.\n",
    "\n",
    "Esto depende en gran medida del indicador y del modelo que esté utilizando. Generalmente, desea que sea cualquier token que use en el mensaje para indicar el inicio de una observación (de lo contrario, el LLM puede alucinar una observación para usted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el Agente\n",
    "---\n",
    "\n",
    "Ahora podemos combinar todo para configurar nuestro agente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilice el agente\n",
    "---\n",
    "\n",
    "¡Ahora podemos usarlo!\n",
    "\n",
    "Comencemos por crear el \"ejecutor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos una pregunta a nuestro agente, por ejemplo preguntemos ¿cuantas persoas viven en canada en el año 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out how many people live in Canada\n",
      "Action: Search\n",
      "Action Input: Population of Canada 2023\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m40,097,761\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Arrr, there be 40,097,761 people livin' in Canada as of 2023!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Arrr, there be 40,097,761 people livin' in Canada as of 2023!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"How many people live in Canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora preguntemos cuantas personas viven en mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to know what's going on in Mexico\n",
      "Action: Search\n",
      "Action Input: Mexico\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m[{'title': 'Migrants are being raped at Mexico border as they await entry to US', 'link': 'https://www.reuters.com/world/migrants-are-being-raped-mexico-border-they-await-entry-us-2023-09-29/', 'source': 'Reuters', 'date': '5 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTP5MpNYEheazJJ9mEuq1fW5vEXgOpfAmRuIKcnXHGv7cTJ8jfBVP0v8p7T-A&usqp=CAI&s=10'}, {'title': 'Turmoil Over Migrants at U.S.-Mexico Border Is Straining Trade Flows', 'link': 'https://www.wsj.com/articles/turmoil-over-migrants-at-u-s-mexico-border-is-straining-trade-flows-3549ebf9', 'source': 'The Wall Street Journal', 'date': '11 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR_Jyj7Z5u4FXKMuQF_SuLtTsj_JESF9N-JjZmsMKY&usqp=CAI&s'}, {'title': 'US, Mexican Officials Talk About Trade, Economic Growth, Border Security', 'link': 'https://www.voanews.com/a/us-mexican-officials-talk-about-trade-economic-growth-border-security/7290762.html', 'source': 'VOA', 'date': '17 mins ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOCKK3wAOTqWAqK6DC82yqhf7bTwshUNtBX-bZ3xBaF8CuJNtqyct_mUY&usqp=CAI&s'}, {'title': \"Monterrey gangs: Bodies dumped in Mexico's business capital\", 'link': 'https://www.bbc.com/news/world-latin-america-66911902', 'source': 'BBC', 'date': '2 days ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQF8Sx0St8RFpbK8GCv_tC57nLoRbcptChaBnYXyE094dsfdJFFoe-h54ES&usqp=CAI&s'}, {'title': 'As many as 12 bodies found strewn around Monterrey in northern Mexico', 'link': 'https://www.aljazeera.com/news/2023/9/27/as-many-as-12-bodies-found-strewn-around-monterrey-in-northern-mexico', 'source': 'Al Jazeera', 'date': '2 days ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSE60XKcEU6jU-aZBjcZeZqDnKMhcVp2h9D40xPBT15IAfay4l_QAAilEA&usqp=CAI&s'}, {'title': \"Mexico's president slams US aid for Ukraine and sanctions on Venezuela and Cuba\", 'link': 'https://apnews.com/article/mexico-ukraine-immigration-criticism-c85668f69ac7c238c63d53b670b7e371', 'source': 'AP News', 'date': '2 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRhgBENHqBa2sF9jo4B95_Nz7FwOm_Ue6XaxDnlYXV1Mp4tL1T_95V3sX0&usqp=CAI&s'}, {'title': 'Have Republicans Learned Nothing From the War on Terror?', 'link': 'https://www.theatlantic.com/ideas/archive/2023/09/us-military-intervention-mexico-fentanyl-crisis/675487/', 'source': 'The Atlantic', 'date': '7 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdBpVlSQWljjBoC_VJKWQBpw1vkDS4hiTU3lgKI_AZDN8g0dgWEwPhH38&usqp=CAI&s'}]\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIt looks like there's a lot of turmoil at the US-Mexico border, and Mexico's president is criticizing US aid and sanctions.\n",
      "Final Answer: Arg, it looks like there be a lot of trouble at the US-Mexico border, and Mexico's president be criticizin' US aid and sanctions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Arg, it looks like there be a lot of trouble at the US-Mexico border, and Mexico's president be criticizin' US aid and sanctions.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"how about in mexico?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar al tener un agente \"sin estado\", no comprende la pregunta en la conversacion.\n",
    "\n",
    "Tendriamos que preguntar directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out how many people live in Mexico\n",
      "Action: Search\n",
      "Action Input: population of Mexico in 2023\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m128,455,567\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThat's a lot of people, I should double check\n",
      "Action: Search\n",
      "Action Input: population of Mexico in 2023\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m128,455,567\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThat's the same number, it must be right\n",
      "Final Answer: Arrr, there be 128,455,567 people livin' in Mexico as of 2023!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Arrr, there be 128,455,567 people livin' in Mexico as of 2023!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"how many people live in mexico as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Holochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
