{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente LLM Personalizado (con un modelo de Chat)\n",
    "---\n",
    "\n",
    "Este cuaderno explica cómo crear su propio agente personalizado basado en un modelo de chat. \n",
    "\n",
    "Un agente de chat LLM consta de tres partes:\n",
    "\n",
    "- `PromptTemplate`: Esta es la plantilla de indicaciones que se puede utilizar para indicarle al modelo de lenguaje qué hacer\n",
    "- `ChatModel`: Este es el modelo de lenguaje que impulsa al agente.\n",
    "- `stop sequence`: Le indica al LLM que deje de generar tan pronto como se encuentre esta cadena\n",
    "- `OutputParser`: Esto determina cómo analizar la salida de `LLM` en un objeto `AgentAction` o `AgentFinish`.\n",
    "\n",
    "El Agente `LLM` se utiliza en un `AgentExecutor`. Este `AgentExecutor` puede considerarse en gran medida como un bucle que:\n",
    "\n",
    "1. Pasa la entrada del usuario y cualquier paso previo al Agente (en este caso, el Agente LLM)\n",
    "2. Si el Agente devuelve un `AgentFinish`, devuélvalo directamente al usuario.\n",
    "3. Si el Agente devuelve una `AgentAction`, úsela para llamar a una herramienta (`tool`) y obtener una Observación (`Observation`)\n",
    "4. Repita, pasando `AgentAction` y `Observation` nuevamente al Agente hasta que se emita un `AgentFinish`.\n",
    "\n",
    "`AgentAction` es una respuesta que consta de `action` y `action_input`. \n",
    "\n",
    "- `action` se refiere a qué herramienta usar \n",
    "- `action_input` se refiere a la entrada a esa herramienta. \n",
    "- `log` también se puede proporcionar como más contexto (que se puede usar para registrar, rastrear, etc.).\n",
    "\n",
    "`AgentFinish` es una respuesta que contiene el mensaje final que se enviará al usuario. Esto debe usarse para finalizar la ejecución de un agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion del Entorno\n",
    "---\n",
    "\n",
    "Realizar las importaciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "from typing import List, Union\n",
    "import re\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar herramienta (tools)\n",
    "---\n",
    "\n",
    "Configure cualquier herramienta (`tool`) que el agente quiera utilizar. \n",
    "\n",
    "Puede ser necesario incluir esto en el `prompt` (para que el agente sepa cómo utilizar estas herramientas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plantilla de Prompt (Prompt Template)\n",
    "---\n",
    "\n",
    "Esto le indica al agente qué hacer. Generalmente, la plantilla debe incorporar:\n",
    "\n",
    "- `tools`: a qué herramientas tiene acceso el agente y cómo y cuándo llamarlas.\n",
    "- `intermediate_steps`: Estas son tuplas de pares anteriores (`AgentAction`, `Observation`). Por lo general, estos no se pasan directamente al modelo, pero el `prompt template` les da formato de una manera específica.\n",
    "- `input`: entrada genérica del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Complete the objective as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "These were previous tasks you completed:\n",
    "\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        \n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizador de salida (Output parser)\n",
    "---\n",
    "\n",
    "El analizador de salida (output parser) es responsable de analizar la salida de LLM en `AgentAction` y `AgentFinish`. Por lo general, esto depende en gran medida del mensaje utilizado.\n",
    "\n",
    "Aquí es donde puede cambiar el análisis para realizar reintentos, manejar espacios en blanco, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el LLM\n",
    "---\n",
    "\n",
    "¡Elija el LLM que desea utilizar!\n",
    "\n",
    "Para este ejemplo usaremos ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir la secuencia de parada (stop sequence)\n",
    "---\n",
    "\n",
    "Esto es importante porque le indica al `LLM` cuándo detener la generación.\n",
    "\n",
    "Esto depende en gran medida del indicador y del modelo que esté utilizando. Generalmente, desea que sea cualquier token que use en el mensaje para indicar el inicio de una observación (de lo contrario, el LLM puede alucinar una observación para usted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar el Agente\n",
    "---\n",
    "\n",
    "Ahora podemos combinar todo para configurar nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilice el agente\n",
    "---\n",
    "\n",
    "¡Ahora podemos usarlo!\n",
    "\n",
    "Comencemos por crear el \"ejecutor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos una pregunta a nuestro agente, por ejemplo preguntemos ¿cuantas persoas viven en canada en el año 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the current population of Canada.\n",
      "Action: Search\n",
      "Action Input: \"current population of Canada 2023\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m38,781,291 people\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the current population of Canada.\n",
      "Final Answer: The current population of Canada as of 2023 is 38,781,291 people.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current population of Canada as of 2023 is 38,781,291 people.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"How many people live in Canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need more context to understand what the question is referring to in Mexico.\n",
      "Action: Search\n",
      "Action Input: \"current events in Mexico\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mFeatured Content\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should click on the Featured Content to get an overview of current events in Mexico.\n",
      "Action: Search\n",
      "Action Input: Click on the Featured Content\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m[\"To get started, click on the Featured Content section in the Customizer. Here you'll be able to set what tag you'll use to label your featured content. Save ...\"]\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis is not the information I was looking for. I need to refine my search.\n",
      "Action: Search\n",
      "Action Input: \"recent news in Mexico\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m[\"Mexico's armed forces sent troops, vehicle convoys and soldiers into remote towns near the southern border with Guatemala, after drug cartels blocked roads and ...\"]\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis article seems to have relevant information about recent events in Mexico.\n",
      "Action: Search\n",
      "Action Input: Click on the article about drug cartels blocking roads near the southern border with Guatemala\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m['... near the southern border with Guatemala on Wednesday after drug cartels blocked roads and cut off electricity in some areas over the weekend.']\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis article provides information about drug cartels blocking roads near the southern border with Guatemala in Mexico.\n",
      "Thought: I now know the current events in Mexico related to drug cartels blocking roads near the southern border with Guatemala.\n",
      "Final Answer: Drug cartels have been blocking roads near the southern border with Guatemala in Mexico.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Drug cartels have been blocking roads near the southern border with Guatemala in Mexico.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"how about in mexico?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar el agente al no tener \"estado\", no comprende la pregunta en la conversacion.\n",
    "\n",
    "Tendriamos que hacer la pregunta directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the current population of Mexico.\n",
      "Action: Search\n",
      "Action Input: \"current population of Mexico 2023\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mThe current population of Mexico is 128,689,862 as of Sunday, October 1, 2023, based on Worldometer elaboration of the latest United Nations data 1.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know the current population of Mexico.\n",
      "Final Answer: The current population of Mexico as of 2023 is 128,689,862.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current population of Mexico as of 2023 is 128,689,862.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"how many people live in mexico as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Holochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
