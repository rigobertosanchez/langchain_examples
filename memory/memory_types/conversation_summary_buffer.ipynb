{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Summary Buffer\n",
    "---\n",
    "`ConversationSummaryBuffer` combina las dos ideas. Mantiene un buffer de interacciones recientes en la memoria, pero en lugar de simplemente eliminar por completo las interacciones antiguas, las compila en un resumen y usa ambas. Utiliza la longitud del token en lugar del número de interacciones para determinar cuándo eliminar las interacciones.\n",
    "\n",
    "Primero veamos cómo usar las utilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la \"memoria\" con un LLM\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\":\"hi\"},\n",
    "    outputs={\"output\": \"whats up\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"not much you\"},\n",
    "    outputs={\"output\": \"not much\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el contenido de la memoria, emplearemos el metodo `load_memory_variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and the AI responds by asking what is up.\\nHuman: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos obtener el historial como una lista de mensajes (esto es útil si lo estás usando con un modelo de chat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\":\"hi\"},\n",
    "    outputs={\"output\": \"whats up\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"not much you\"},\n",
    "    outputs={\"output\": \"not much\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual forma, veamos el contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"\\nThe human greets the AI and the AI responds by asking what's up.\", additional_kwargs={}),\n",
       "  HumanMessage(content='not much you', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='not much', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "También podemos utilizar el método `predict_new_summary` directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good. They then ask each other how they are doing, with both responding that not much is happening.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "previous_sumamry = \"\"\n",
    "memory.predict_new_summary(messages, previous_sumamry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando en un \"chain\"\n",
    "---\n",
    "\n",
    "Veamos un ejemplo, configurando nuevamente `verbose=True` para que podamos ver el mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great. I'm currently working on an artificial intelligence project that involves natural language processing. I'm trying to get better at understanding how humans communicate with each other. How about you?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=OpenAI(),\n",
    "    max_token_limit=40,\n",
    ")\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Lets make a test\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, stating that it is working on an artificial intelligence project involving natural language processing to better understand how humans communicate.\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ah, I see. That sounds like a very interesting and important project. Natural language processing is a fascinating field of study. Have you been working on this project for long?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, stating that it is working on an artificial intelligence project involving natural language processing to better understand how humans communicate. The human reveals that they are just working on writing some documentation.\n",
      "AI:  Ah, I see. That sounds like a very interesting and important project. Natural language processing is a fascinating field of study. Have you been working on this project for long?\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes, I have heard of LangChain. I understand that it is a platform for language learning, which aims to make it easier for people to learn new languages. Is that correct?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that there is a summary of the conversation and then some previous interactions\n",
    "conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds, stating that it is working on an artificial intelligence project involving natural language processing to better understand how humans communicate. The human reveals that they are just working on writing some documentation for LangChain, to which the AI expresses its interest in natural language processing and inquires as to how long the human has been working on the project.\n",
      "AI:  Yes, I have heard of LangChain. I understand that it is a platform for language learning, which aims to make it easier for people to learn new languages. Is that correct?\n",
      "Human: Haha nope, although a lot of people confuse it for that\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Oh, I see. So, what is LangChain then?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that the summary and the buffer are updated\n",
    "conversation_with_summary.predict(\n",
    "    input=\"Haha nope, although a lot of people confuse it for that\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Holochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
