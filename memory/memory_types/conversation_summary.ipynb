{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Summary\n",
    "---\n",
    "\n",
    "Veamos otro tipo de memoria, esta es un poco mas compleja, `ConversationSummaryMemory`. Este tipo de memoria crea un resumen de la conversacion a lo largo del tiempo.\n",
    "\n",
    "Esto puede resultar util para condensar la informacion de la conversacion. La memoria va siendo resumida a memdida que ocurre y almacena el resumen actual en la memoria.\n",
    "\n",
    "Esta memoria se puede utilizar para inyectar el resumen de la conversacion hasta el momento en un mensaje.\n",
    "\n",
    "Este tipo de memoria es mas util para conversaciones mas largas donde mantener el historial de mensajes pasados en el mensaje palabra por palabra consumiria demasiados tokkens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad basica\n",
    "---\n",
    "Hagamos unos ejemplos con su forma mas basica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizemos el primer ejemplo, instanciando la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# memory\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Add context to memory\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"hi\"},\n",
    "    outputs={\"output\": \"whats up\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar el contenido de la `memoria` usaremos el metodo `load_memory_variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human greets the AI and the AI responds with a casual greeting.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener el historial como una lista de mensajes (recordemos que esto es util para cuando usamos un modelo de \"chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# memory\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# Add context to memory\n",
    "memory.save_context(\n",
    "    inputs={\"input\": \"hi\"},\n",
    "    outputs={\"output\": \"whats up\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual forma, obtenemos la `memoria`, y usaremos para esto el metodo `load_memory_variables`, a diferencia del ejemplo anterior observaremos los mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='\\nThe human greets the AI and the AI responds with a casual greeting.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de obtener el `resumen`es usar el metodo `predict_new_summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human greets the AI, to which the AI responds.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "\n",
    "# Para que no se agregue mas informacion al resumen, enviaremos una cadena vacia\n",
    "previous_summary=\"\"\n",
    "\n",
    "# Empleamos el metodo para obtener el resumen\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando con mensajes o resumen\n",
    "---\n",
    "\n",
    "Si contamos con mensajes fuera de la clase, podemos inicializar facilmente con la clase `ChatMessageHistory`. \n",
    "\n",
    "Durante la carga, un resumen sera calculado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, hagamos un ejemplo cargando mediante `ChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"Hi\")\n",
    "history.add_ai_message(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciemos la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=llm,\n",
    "    chat_memory=history,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora el buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human greets the AI, to which the AI responds with a friendly greeting.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcionalmente, puede acelerar la inicialización utilizando un resumen generado previamente y evitar volver a generar el resumen simplemente inicializándolo directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    buffer=\"The human asks what the AI thinks of artificial intelligente. The AI thinks artificial intelligente is a force for good because it will help humans reach their full potential.\",\n",
    "    chat_memory=history,\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human asks what the AI thinks of artificial intelligente. The AI thinks artificial intelligente is a force for good because it will help humans reach their full potential.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human asks what the AI thinks of artificial intelligente. The AI thinks artificial intelligente is a force for good because it will help humans reach their full potential.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso en un \"chain\"\n",
    "---\n",
    "\n",
    "Veamos un ejemplo usando este tipo de memoria en un \"chain\", usaremos la configuracion `verbose=True`, para que podamos observar el `prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# llm\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# summary memory\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# conversation chain\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human greets the AI and the AI responds that it is doing great and helping a customer with a technical issue.\n",
      "Human: Tell me more about it!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure! The customer is having trouble with their computer not connecting to the internet. I'm helping them troubleshoot the issue and figure out what the problem is. So far, I've been able to narrow it down to a few possible causes, but I'm still trying to determine the exact cause.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Tell me more about it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human greets the AI and the AI responds that it is doing great and helping a customer with a technical issue. The customer is having trouble with their computer not connecting to the internet and the AI is helping them troubleshoot the issue and figure out what the problem is. So far, the AI has been able to narrow it down to a few possible causes, but is still trying to determine the exact cause.\n",
      "Human: Very cool -- what is the scope of the project?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The scope of the project is to help the customer troubleshoot their computer's internet connection issue. I'm currently in the process of narrowing down the possible causes and trying to determine the exact cause of the issue.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Very cool -- what is the scope of the project?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Holochat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
